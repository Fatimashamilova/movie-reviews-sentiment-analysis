{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dbd7db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"movie_reviews_db\"]\n",
    "collection = db[\"movie_reviews\"]\n",
    "\n",
    "data = pd.read_csv(\"/Users/fatimashamilova/movie_review.csv\")\n",
    "\n",
    "collection.insert_many(data.to_dict(\"records\"))\n",
    "\n",
    "print(\"Data inserted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "539e3835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping sklearn as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25566af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./anaconda3/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./anaconda3/lib/python3.11/site-packages (from scikit-learn) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "227a9cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in ./anaconda3/lib/python3.11/site-packages (4.10.1)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in ./anaconda3/lib/python3.11/site-packages (from pymongo) (2.7.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8d42dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('67466a7f0a17b006abdb0350'), 'fold_id': 0, 'cv_tag': 'cv000', 'html_id': 29590, 'sent_id': 0, 'text': \"films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before .\", 'tag': 'pos'}\n",
      "{'_id': ObjectId('67466a7f0a17b006abdb0351'), 'fold_id': 0, 'cv_tag': 'cv000', 'html_id': 29590, 'sent_id': 1, 'text': \"for starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid '80s with a 12-part series called the watchmen .\", 'tag': 'pos'}\n",
      "{'_id': ObjectId('67466a7f0a17b006abdb0352'), 'fold_id': 0, 'cv_tag': 'cv000', 'html_id': 29590, 'sent_id': 2, 'text': 'to say moore and campbell thoroughly researched the subject of jack the ripper would be like saying michael jackson is starting to look a little odd .', 'tag': 'pos'}\n",
      "{'_id': ObjectId('67466a7f0a17b006abdb0353'), 'fold_id': 0, 'cv_tag': 'cv000', 'html_id': 29590, 'sent_id': 3, 'text': 'the book ( or \" graphic novel , \" if you will ) is over 500 pages long and includes nearly 30 more that consist of nothing but footnotes .', 'tag': 'pos'}\n",
      "{'_id': ObjectId('67466a7f0a17b006abdb0354'), 'fold_id': 0, 'cv_tag': 'cv000', 'html_id': 29590, 'sent_id': 4, 'text': \"in other words , don't dismiss this film because of its source .\", 'tag': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "for review in collection.find().limit(5):  \n",
    "    print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b229e9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['_id', 'fold_id', 'cv_tag', 'html_id', 'sent_id', 'text', 'tag'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc = collection.find_one() \n",
    "print(doc.keys())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91572e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text Data: [\"films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before .\", \"for starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid '80s with a 12-part series called the watchmen .\", 'to say moore and campbell thoroughly researched the subject of jack the ripper would be like saying michael jackson is starting to look a little odd .', 'the book ( or \" graphic novel , \" if you will ) is over 500 pages long and includes nearly 30 more that consist of nothing but footnotes .', \"in other words , don't dismiss this film because of its source .\"]\n",
      "Sample Tags: ['pos', 'pos', 'pos', 'pos', 'pos']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "documents = [doc['text'] for doc in collection.find()]  \n",
    "labels = [doc['tag'] for doc in collection.find()]     \n",
    "\n",
    "\n",
    "print(\"Sample Text Data:\", documents[:5])  \n",
    "print(\"Sample Tags:\", labels[:5])         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b51d0b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Matrix Shape: (129440, 39659)\n",
      "Sample BoW Vector (First Document): [[0 0 0 ... 0 0 0]]\n",
      "Feature Names (First 10 Words): ['00' '000' '0009f' '007' '00s' '03' '04' '05' '05425' '10']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow_vectorizer = CountVectorizer()\n",
    "\n",
    "bow_matrix = bow_vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"BoW Matrix Shape:\", bow_matrix.shape)  \n",
    "print(\"Sample BoW Vector (First Document):\", bow_matrix[0].toarray())\n",
    "print(\"Feature Names (First 10 Words):\", bow_vectorizer.get_feature_names_out()[:10])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6a8eaec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix Shape: (5, 61)\n",
      "Feature Names (First 10 Words): ['80s' 'adapted' 'alan' 'arthouse' 'batman' 'book' 'books' 'brought'\n",
      " 'called' 'campbell']\n",
      "TF-IDF Term-Document Matrix:\n",
      "        80s  adapted      alan  arthouse   batman      book    books  \\\n",
      "0  0.000000  0.20716  0.000000   0.20716  0.20716  0.167135  0.20716   \n",
      "1  0.264426  0.00000  0.264426   0.00000  0.00000  0.000000  0.00000   \n",
      "2  0.000000  0.00000  0.000000   0.00000  0.00000  0.000000  0.00000   \n",
      "3  0.000000  0.00000  0.000000   0.00000  0.00000  0.274304  0.00000   \n",
      "4  0.000000  0.00000  0.000000   0.00000  0.00000  0.000000  0.00000   \n",
      "\n",
      "    brought    called  campbell  ...  starters  starting   subject  success  \\\n",
      "0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.20716   \n",
      "1  0.264426  0.264426  0.213337  ...  0.264426  0.000000  0.000000  0.00000   \n",
      "2  0.000000  0.000000  0.208642  ...  0.000000  0.258607  0.258607  0.00000   \n",
      "3  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.00000   \n",
      "4  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.00000   \n",
      "\n",
      "   superheroes  superman  thoroughly  watchmen  words    world  \n",
      "0      0.20716   0.20716    0.000000  0.000000    0.0  0.20716  \n",
      "1      0.00000   0.00000    0.000000  0.264426    0.0  0.00000  \n",
      "2      0.00000   0.00000    0.258607  0.000000    0.0  0.00000  \n",
      "3      0.00000   0.00000    0.000000  0.000000    0.0  0.00000  \n",
      "4      0.00000   0.00000    0.000000  0.000000    0.5  0.00000  \n",
      "\n",
      "[5 rows x 61 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/fatimashamilova/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fatimashamilova/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "dataset = [\n",
    "    \"films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before .\",\n",
    "    \"for starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid '80s with a 12-part series called the watchmen .\",\n",
    "    \"to say moore and campbell thoroughly researched the subject of jack the ripper would be like saying michael jackson is starting to look a little odd .\",\n",
    "    \"the book ( or ' graphic novel , ' if you will ) is over 500 pages long and includes nearly 30 more that consist of nothing but footnotes .\",\n",
    "    \"in other words , don't dismiss this film because of its source .\"\n",
    "]\n",
    "\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i] = dataset[i].lower()\n",
    "    dataset[i] = re.sub(r'\\W', ' ', dataset[i])\n",
    "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i])\n",
    "\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "for i in range(len(dataset)):\n",
    "    words = nltk.word_tokenize(dataset[i])\n",
    "    words = [word for word in words if word not in stopwords and not word.isdigit()]\n",
    "    dataset[i] = ' '.join(words)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "tfidf_vect = tfidf_vectorizer.fit_transform(dataset)\n",
    "\n",
    "print(\"TF-IDF Matrix Shape:\", tfidf_vect.shape)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "tfidf_tdm = pd.DataFrame(tfidf_vect.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "print(\"Feature Names (First 10 Words):\", tfidf_vectorizer.get_feature_names_out()[:10])\n",
    "\n",
    "\n",
    "print(\"TF-IDF Term-Document Matrix:\")\n",
    "print(tfidf_tdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8720ce82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/fatimashamilova/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fatimashamilova/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.71      0.66      0.69      6371\n",
      "         pos       0.70      0.74      0.72      6573\n",
      "\n",
      "    accuracy                           0.70     12944\n",
      "   macro avg       0.71      0.70      0.70     12944\n",
      "weighted avg       0.71      0.70      0.70     12944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client[\"nlp_project_db\"]\n",
    "collection = db[\"movie_reviews\"]\n",
    "\n",
    "\n",
    "data = pd.DataFrame(collection.find({}, {\"text\": 1, \"tag\": 1}))\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [word for word in words if word not in nltk.corpus.stopwords.words('english') and not word.isdigit()]\n",
    "    return ' '.join(words)\n",
    "\n",
    "data[\"processed_text\"] = data[\"text\"].apply(preprocess_text)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(data[\"processed_text\"])\n",
    "y = data[\"tag\"]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e22e144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: ['pos' 'neg' 'pos' 'neg' 'neg']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "print(\"Predictions:\", y_pred[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8b8e850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.71      0.66      0.69      6371\n",
      "         pos       0.70      0.74      0.72      6573\n",
      "\n",
      "    accuracy                           0.70     12944\n",
      "   macro avg       0.71      0.70      0.70     12944\n",
      "weighted avg       0.71      0.70      0.70     12944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce7d104b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in ./anaconda3/lib/python3.11/site-packages (4.10.1)\n",
      "Requirement already satisfied: dnspython in ./anaconda3/lib/python3.11/site-packages (2.7.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymongo dnspython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5b9ddbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/fatimashamilova/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fatimashamilova/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  films adapted from comic books have had plenty...   \n",
      "1  for starters , it was created by alan moore ( ...   \n",
      "2  to say moore and campbell thoroughly researche...   \n",
      "3  the book ( or \" graphic novel , \" if you will ...   \n",
      "4  in other words , don't dismiss this film becau...   \n",
      "\n",
      "                                      processed_text  \n",
      "0  films adapted comic books plenty success wheth...  \n",
      "1  starters created alan moore eddie campbell bro...  \n",
      "2  say moore campbell thoroughly researched subje...  \n",
      "3  book graphic novel 500 pages long includes nea...  \n",
      "4                          words dismiss film source  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower() \n",
    "    text = re.sub(r'\\W', ' ', text)  \n",
    "    text = re.sub(r'\\s+', ' ', text) \n",
    "    words = nltk.word_tokenize(text)  \n",
    "    words = [word for word in words if word not in stop_words] \n",
    "    return ' '.join(words)\n",
    "\n",
    "data['processed_text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "print(data[['text', 'processed_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e78023e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    00  000  0009f  007  00s   03   04   05  05425   10  ...  zukovsky  zulu  \\\n",
      "0  0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  ...       0.0   0.0   \n",
      "1  0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  ...       0.0   0.0   \n",
      "2  0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  ...       0.0   0.0   \n",
      "3  0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  ...       0.0   0.0   \n",
      "4  0.0  0.0    0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  ...       0.0   0.0   \n",
      "\n",
      "   zundel  zurg  zus  zweibel  zwick  zwigoff  zycie  zzzzzzz  \n",
      "0     0.0   0.0  0.0      0.0    0.0      0.0    0.0      0.0  \n",
      "1     0.0   0.0  0.0      0.0    0.0      0.0    0.0      0.0  \n",
      "2     0.0   0.0  0.0      0.0    0.0      0.0    0.0      0.0  \n",
      "3     0.0   0.0  0.0      0.0    0.0      0.0    0.0      0.0  \n",
      "4     0.0   0.0  0.0      0.0    0.0      0.0    0.0      0.0  \n",
      "\n",
      "[5 rows x 39511 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['processed_text'])\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab3925d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some Collection: Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'MovieReviewDB'), 'movie_review')\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "#-*- coding: utf-8 -*-\n",
    "\n",
    "# import the MongoClient class from the library\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# create an client instance of the MongoDB class\n",
    "mongo_client = MongoClient()\n",
    "\n",
    "# create an instance of 'some_database'\n",
    "db = mongo_client.MovieReviewDB\n",
    "\n",
    "\n",
    "# access a MongoDB collection name with spaces\n",
    "col = db[\"movie_review\"]\n",
    "print (\"Some Collection:\", col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151be1c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
